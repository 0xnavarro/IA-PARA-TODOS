##  Ajuste Fino de Llama 3.2 en 30 L铆neas de Python

Este script demuestra c贸mo realizar el ajuste fino del modelo Llama 3.2 usando la biblioteca [Unsloth](https://unsloth.ai/), que hace el proceso f谩cil y r谩pido. Puedes ejecutar este ejemplo para ajustar los modelos Llama 3.1 1B y 3B de forma gratuita en Google Colab.

### Caracter铆sticas

- Ajusta el modelo Llama 3.2 usando la biblioteca Unsloth
- Implementa Adaptaci贸n de Bajo Rango (LoRA) para un ajuste eficiente
- Utiliza el conjunto de datos FineTome-100k para el entrenamiento
- Configurable para diferentes tama帽os de modelo (1B y 3B)

### Instalaci贸n

1. Clona el repositorio:

```bash
git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git
cd llama3.2_finetuning
```

2. Instala las dependencias requeridas:

```bash
pip install -r requirements.txt
```

## Uso

1. Abre el script en Google Colab o tu entorno Python preferido.

2. Ejecuta el script para iniciar el proceso de ajuste fino:

```bash
# Ejecutar el script completo
python finetune_llama3.2.py
```

3. El modelo ajustado se guardar谩 en el directorio "finetuned_model".

## C贸mo Funciona

1. **Carga del Modelo**: El script carga el modelo Llama 3.2 3B Instruct usando FastLanguageModel de Unsloth.

2. **Configuraci贸n de LoRA**: Se aplica la Adaptaci贸n de Bajo Rango a capas espec铆ficas del modelo para un ajuste eficiente.

3. **Preparaci贸n de Datos**: El conjunto de datos FineTome-100k se carga y preprocesa usando una plantilla de chat.

4. **Configuraci贸n del Entrenamiento**: El script configura el SFTTrainer con argumentos espec铆ficos de entrenamiento.

5. **Ajuste Fino**: El modelo se ajusta con el conjunto de datos preparado.

6. **Guardado del Modelo**: El modelo ajustado se guarda en disco.

## Configuraci贸n

Puedes modificar los siguientes par谩metros en el script:

- `model_name`: Cambiar a "unsloth/Llama-3.1-1B-Instruct" para el modelo 1B
- `max_seq_length`: Ajustar la longitud m谩xima de secuencia
- `r`: Rango de LoRA
- Hiperpar谩metros de entrenamiento en `TrainingArguments`

## Personalizaci贸n

- Para usar un conjunto de datos diferente, reemplaza la llamada a la funci贸n `load_dataset` con tu conjunto de datos deseado.
- Ajusta los `target_modules` en la configuraci贸n de LoRA para ajustar diferentes capas del modelo.
- Modifica la plantilla de chat en `get_chat_template` si est谩s usando un formato de conversaci贸n diferente.

## Ejecuci贸n en Google Colab

1. Abre un nuevo cuaderno de Google Colab.
2. Copia el script completo en una celda de c贸digo.
3. Agrega una celda al principio para instalar las bibliotecas requeridas:

```
!pip install torch transformers datasets trl unsloth
```

4. Ejecuta las celdas para iniciar el proceso de ajuste fino.

## Notas

- Este script est谩 optimizado para ejecutarse en la versi贸n gratuita de Google Colab, que proporciona acceso a GPUs.
- El proceso de ajuste fino puede llevar alg煤n tiempo, dependiendo del tama帽o del modelo y los recursos computacionales disponibles.
- Aseg煤rate de tener suficiente espacio de almacenamiento en tu instancia de Colab para guardar el modelo ajustado.